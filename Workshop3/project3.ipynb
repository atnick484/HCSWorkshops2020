{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "2.7.16-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python2",
   "display_name": "Python 2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifies mushrooms to be either poisonous or edible based on other attributes\n",
    "# data from: https://www.kaggle.com/uciml/mushroom-classification\n",
    "\n",
    "# import required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import math\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "data = pd.read_csv('./mushrooms.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate into poisonous and edible \n",
    "data_poison = data.loc[data['class'] == 'p']\n",
    "data_edible = data.loc[data['class'] == 'e']\n",
    "\n",
    "# get rid of class column (which tells if it is edible or poisonous)\n",
    "data_poison.drop('class', inplace=True, axis=1)\n",
    "data_edible.drop('class', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# convert to numpy\n",
    "data_npy_poison = data_poison.to_numpy()\n",
    "data_npy_edible = data_edible.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[['x' 's' 'n' ... 'k' 's' 'u']\n ['x' 'y' 'w' ... 'k' 's' 'u']\n ['x' 'y' 'w' ... 'k' 'v' 'g']\n ...\n ['k' 's' 'e' ... 'w' 'v' 'd']\n ['k' 'y' 'n' ... 'w' 'v' 'd']\n ['k' 'y' 'n' ... 'w' 'v' 'l']]\n"
     ]
    }
   ],
   "source": [
    "print(data_npy_poison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[['f' 'y' 'y' ... 'h' 'v' 'g']\n ['x' 'y' 'n' ... 'w' 'v' 'p']\n ['k' 'y' 'e' ... 'w' 'v' 'p']\n ...\n ['x' 'y' 'r' ... 'h' 'v' 'd']\n ['x' 'f' 'n' ... 'k' 'a' 'g']\n ['x' 'f' 'n' ... 'n' 'v' 'd']]\n"
     ]
    }
   ],
   "source": [
    "# randomizing the data\n",
    "np.random.shuffle(data_npy_poison)\n",
    "np.random.shuffle(data_npy_edible)\n",
    "\n",
    "# splitting into test and train\n",
    "train_percent = 0.8\n",
    "\n",
    "poison_cutoff = int(math.floor(len(data_npy_poison) * train_percent))\n",
    "poison_trainX = data_npy_poison[:poison_cutoff]\n",
    "poison_trainY = np.tile([1,0,0,0], (len(poison_trainX),1))\n",
    "poison_testX = data_npy_poison[poison_cutoff:]\n",
    "poison_testY = np.tile([1,0,0,0], (len(poison_testX),1))\n",
    "\n",
    "edible_cutoff = int(math.floor(len(data_npy_edible) * train_percent))\n",
    "edible_trainX = data_npy_edible[:edible_cutoff]\n",
    "edible_trainY = np.tile([0,1,0,0], (len(edible_trainX),1))\n",
    "edible_testX = data_npy_edible[edible_cutoff:]\n",
    "edible_testY = np.tile([0,1,0,0], (len(edible_testX),1))\n",
    "\n",
    "# building the training and testing arrays\n",
    "\n",
    "X_train = np.concatenate((poison_trainX, edible_trainX))\n",
    "\n",
    "Y_train = np.concatenate((poison_trainY, edible_trainY))\n",
    "\n",
    "X_test = np.concatenate((poison_testX, edible_testX))\n",
    "Y_test = np.concatenate((poison_testY, edible_testY))\n",
    "\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the X data to floats and normalize the X data\n",
    "floaterize = lambda t: float(ord(t)) / 121\n",
    "vfunc = np.vectorize(floaterize) \n",
    "\n",
    "X_train = vfunc(X_train)\n",
    "X_test = vfunc(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0.84297521 1.         1.         ... 0.85950413 0.97520661 0.85123967]\n [0.99173554 1.         0.90909091 ... 0.98347107 0.97520661 0.92561983]\n [0.88429752 1.         0.83471074 ... 0.98347107 0.97520661 0.92561983]\n ...\n [0.99173554 1.         0.94214876 ... 0.85950413 0.97520661 0.82644628]\n [0.99173554 0.84297521 0.90909091 ... 0.88429752 0.80165289 0.85123967]\n [0.99173554 0.84297521 0.90909091 ... 0.90909091 0.97520661 0.82644628]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building out the model\n",
    "model = keras.models.Sequential()\n",
    "\n",
    "# adding the first hidden layer\n",
    "model.add(layers.Dense(512, input_shape=(22,)))\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(layers.Dropout(0.2))\n",
    "\n",
    "# adding the second hidden layer\n",
    "model.add(layers.Dense(512))\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(layers.Dropout(0.2))\n",
    "\n",
    "# adding the output layer (four outputs)\n",
    "model.add(layers.Dense(4))\n",
    "model.add(layers.Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling the model\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train on 6498 samples, validate on 1626 samples\n",
      "Epoch 1/20\n",
      "6498/6498 - 0s - loss: 0.7068 - accuracy: 0.5620 - val_loss: 0.6217 - val_accuracy: 0.6661\n",
      "Epoch 2/20\n",
      "6498/6498 - 0s - loss: 0.5912 - accuracy: 0.6931 - val_loss: 0.5096 - val_accuracy: 0.7355\n",
      "Epoch 3/20\n",
      "6498/6498 - 0s - loss: 0.5119 - accuracy: 0.7562 - val_loss: 0.4325 - val_accuracy: 0.8284\n",
      "Epoch 4/20\n",
      "6498/6498 - 0s - loss: 0.4256 - accuracy: 0.8216 - val_loss: 0.3979 - val_accuracy: 0.8438\n",
      "Epoch 5/20\n",
      "6498/6498 - 0s - loss: 0.3788 - accuracy: 0.8473 - val_loss: 0.3241 - val_accuracy: 0.8899\n",
      "Epoch 6/20\n",
      "6498/6498 - 0s - loss: 0.3293 - accuracy: 0.8766 - val_loss: 0.2823 - val_accuracy: 0.8930\n",
      "Epoch 7/20\n",
      "6498/6498 - 0s - loss: 0.2985 - accuracy: 0.8914 - val_loss: 0.2942 - val_accuracy: 0.8856\n",
      "Epoch 8/20\n",
      "6498/6498 - 0s - loss: 0.2878 - accuracy: 0.8926 - val_loss: 0.2664 - val_accuracy: 0.9164\n",
      "Epoch 9/20\n",
      "6498/6498 - 0s - loss: 0.2943 - accuracy: 0.8917 - val_loss: 0.2507 - val_accuracy: 0.9231\n",
      "Epoch 10/20\n",
      "6498/6498 - 0s - loss: 0.2536 - accuracy: 0.9137 - val_loss: 0.2365 - val_accuracy: 0.9256\n",
      "Epoch 11/20\n",
      "6498/6498 - 0s - loss: 0.2549 - accuracy: 0.9104 - val_loss: 0.2485 - val_accuracy: 0.9280\n",
      "Epoch 12/20\n",
      "6498/6498 - 0s - loss: 0.2604 - accuracy: 0.9086 - val_loss: 0.2253 - val_accuracy: 0.9256\n",
      "Epoch 13/20\n",
      "6498/6498 - 0s - loss: 0.2449 - accuracy: 0.9161 - val_loss: 0.2296 - val_accuracy: 0.9360\n",
      "Epoch 14/20\n",
      "6498/6498 - 0s - loss: 0.2437 - accuracy: 0.9164 - val_loss: 0.2277 - val_accuracy: 0.9164\n",
      "Epoch 15/20\n",
      "6498/6498 - 0s - loss: 0.2379 - accuracy: 0.9184 - val_loss: 0.3296 - val_accuracy: 0.8708\n",
      "Epoch 16/20\n",
      "6498/6498 - 0s - loss: 0.2342 - accuracy: 0.9180 - val_loss: 0.2156 - val_accuracy: 0.9385\n",
      "Epoch 17/20\n",
      "6498/6498 - 0s - loss: 0.2230 - accuracy: 0.9246 - val_loss: 0.2029 - val_accuracy: 0.9410\n",
      "Epoch 18/20\n",
      "6498/6498 - 0s - loss: 0.2155 - accuracy: 0.9294 - val_loss: 0.2016 - val_accuracy: 0.9434\n",
      "Epoch 19/20\n",
      "6498/6498 - 0s - loss: 0.2042 - accuracy: 0.9301 - val_loss: 0.2102 - val_accuracy: 0.9410\n",
      "Epoch 20/20\n",
      "6498/6498 - 0s - loss: 0.2100 - accuracy: 0.9287 - val_loss: 0.1861 - val_accuracy: 0.9446\n"
     ]
    }
   ],
   "source": [
    "# training the model\n",
    "history = model.fit(X_train, Y_train,\n",
    "          batch_size=128, epochs=20,\n",
    "          verbose=2,\n",
    "          validation_data=(X_test, Y_test))"
   ]
  }
 ]
}
